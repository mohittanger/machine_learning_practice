{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas\n",
    "import pandas as pd\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "#pima = pd.read_csv(\"pima-indians-diabetes.csv\", header=None, names=col_names)\n",
    "pima = pd.read_csv(\"pima-indians-diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "feature_cols = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\n",
    "x = pima[feature_cols] # Features\n",
    "y = pima.Outcome # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>139</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>96</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>126</td>\n",
       "      <td>88</td>\n",
       "      <td>41</td>\n",
       "      <td>235</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.704</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>99</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.4</td>\n",
       "      <td>0.388</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7</td>\n",
       "      <td>196</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.451</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "      <td>80</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.263</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11</td>\n",
       "      <td>143</td>\n",
       "      <td>94</td>\n",
       "      <td>33</td>\n",
       "      <td>146</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>125</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>115</td>\n",
       "      <td>31.1</td>\n",
       "      <td>0.205</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>147</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.4</td>\n",
       "      <td>0.257</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>140</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0.487</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13</td>\n",
       "      <td>145</td>\n",
       "      <td>82</td>\n",
       "      <td>19</td>\n",
       "      <td>110</td>\n",
       "      <td>22.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>117</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.1</td>\n",
       "      <td>0.337</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>60</td>\n",
       "      <td>17</td>\n",
       "      <td>160</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.453</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0.293</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>11</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>37</td>\n",
       "      <td>150</td>\n",
       "      <td>42.3</td>\n",
       "      <td>0.785</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>3</td>\n",
       "      <td>102</td>\n",
       "      <td>44</td>\n",
       "      <td>20</td>\n",
       "      <td>94</td>\n",
       "      <td>30.8</td>\n",
       "      <td>0.400</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>58</td>\n",
       "      <td>18</td>\n",
       "      <td>116</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.219</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>9</td>\n",
       "      <td>140</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>0.734</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>13</td>\n",
       "      <td>153</td>\n",
       "      <td>88</td>\n",
       "      <td>37</td>\n",
       "      <td>140</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1.174</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>84</td>\n",
       "      <td>33</td>\n",
       "      <td>105</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.488</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>94</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>49.3</td>\n",
       "      <td>0.358</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>74</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>46.3</td>\n",
       "      <td>1.096</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>3</td>\n",
       "      <td>187</td>\n",
       "      <td>70</td>\n",
       "      <td>22</td>\n",
       "      <td>200</td>\n",
       "      <td>36.4</td>\n",
       "      <td>0.408</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>6</td>\n",
       "      <td>162</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.178</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>4</td>\n",
       "      <td>136</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.2</td>\n",
       "      <td>1.182</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>78</td>\n",
       "      <td>39</td>\n",
       "      <td>74</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.261</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>3</td>\n",
       "      <td>108</td>\n",
       "      <td>62</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.223</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>88</td>\n",
       "      <td>44</td>\n",
       "      <td>510</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.222</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>8</td>\n",
       "      <td>154</td>\n",
       "      <td>78</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.443</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>88</td>\n",
       "      <td>39</td>\n",
       "      <td>110</td>\n",
       "      <td>36.5</td>\n",
       "      <td>1.057</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>7</td>\n",
       "      <td>137</td>\n",
       "      <td>90</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.391</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.258</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.197</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>6</td>\n",
       "      <td>190</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>0.278</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.766</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>9</td>\n",
       "      <td>170</td>\n",
       "      <td>74</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.403</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>9</td>\n",
       "      <td>89</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.142</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "5              5      116             74              0        0  25.6   \n",
       "6              3       78             50             32       88  31.0   \n",
       "7             10      115              0              0        0  35.3   \n",
       "8              2      197             70             45      543  30.5   \n",
       "9              8      125             96              0        0   0.0   \n",
       "10             4      110             92              0        0  37.6   \n",
       "11            10      168             74              0        0  38.0   \n",
       "12            10      139             80              0        0  27.1   \n",
       "13             1      189             60             23      846  30.1   \n",
       "14             5      166             72             19      175  25.8   \n",
       "15             7      100              0              0        0  30.0   \n",
       "16             0      118             84             47      230  45.8   \n",
       "17             7      107             74              0        0  29.6   \n",
       "18             1      103             30             38       83  43.3   \n",
       "19             1      115             70             30       96  34.6   \n",
       "20             3      126             88             41      235  39.3   \n",
       "21             8       99             84              0        0  35.4   \n",
       "22             7      196             90              0        0  39.8   \n",
       "23             9      119             80             35        0  29.0   \n",
       "24            11      143             94             33      146  36.6   \n",
       "25            10      125             70             26      115  31.1   \n",
       "26             7      147             76              0        0  39.4   \n",
       "27             1       97             66             15      140  23.2   \n",
       "28            13      145             82             19      110  22.2   \n",
       "29             5      117             92              0        0  34.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "738            2       99             60             17      160  36.6   \n",
       "739            1      102             74              0        0  39.5   \n",
       "740           11      120             80             37      150  42.3   \n",
       "741            3      102             44             20       94  30.8   \n",
       "742            1      109             58             18      116  28.5   \n",
       "743            9      140             94              0        0  32.7   \n",
       "744           13      153             88             37      140  40.6   \n",
       "745           12      100             84             33      105  30.0   \n",
       "746            1      147             94             41        0  49.3   \n",
       "747            1       81             74             41       57  46.3   \n",
       "748            3      187             70             22      200  36.4   \n",
       "749            6      162             62              0        0  24.3   \n",
       "750            4      136             70              0        0  31.2   \n",
       "751            1      121             78             39       74  39.0   \n",
       "752            3      108             62             24        0  26.0   \n",
       "753            0      181             88             44      510  43.3   \n",
       "754            8      154             78             32        0  32.4   \n",
       "755            1      128             88             39      110  36.5   \n",
       "756            7      137             90             41        0  32.0   \n",
       "757            0      123             72              0        0  36.3   \n",
       "758            1      106             76              0        0  37.5   \n",
       "759            6      190             92              0        0  35.5   \n",
       "760            2       88             58             26       16  28.4   \n",
       "761            9      170             74             31        0  44.0   \n",
       "762            9       89             62              0        0  22.5   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  \n",
       "0                       0.627   50  \n",
       "1                       0.351   31  \n",
       "2                       0.672   32  \n",
       "3                       0.167   21  \n",
       "4                       2.288   33  \n",
       "5                       0.201   30  \n",
       "6                       0.248   26  \n",
       "7                       0.134   29  \n",
       "8                       0.158   53  \n",
       "9                       0.232   54  \n",
       "10                      0.191   30  \n",
       "11                      0.537   34  \n",
       "12                      1.441   57  \n",
       "13                      0.398   59  \n",
       "14                      0.587   51  \n",
       "15                      0.484   32  \n",
       "16                      0.551   31  \n",
       "17                      0.254   31  \n",
       "18                      0.183   33  \n",
       "19                      0.529   32  \n",
       "20                      0.704   27  \n",
       "21                      0.388   50  \n",
       "22                      0.451   41  \n",
       "23                      0.263   29  \n",
       "24                      0.254   51  \n",
       "25                      0.205   41  \n",
       "26                      0.257   43  \n",
       "27                      0.487   22  \n",
       "28                      0.245   57  \n",
       "29                      0.337   38  \n",
       "..                        ...  ...  \n",
       "738                     0.453   21  \n",
       "739                     0.293   42  \n",
       "740                     0.785   48  \n",
       "741                     0.400   26  \n",
       "742                     0.219   22  \n",
       "743                     0.734   45  \n",
       "744                     1.174   39  \n",
       "745                     0.488   46  \n",
       "746                     0.358   27  \n",
       "747                     1.096   32  \n",
       "748                     0.408   36  \n",
       "749                     0.178   50  \n",
       "750                     1.182   22  \n",
       "751                     0.261   28  \n",
       "752                     0.223   25  \n",
       "753                     0.222   26  \n",
       "754                     0.443   45  \n",
       "755                     1.057   37  \n",
       "756                     0.391   39  \n",
       "757                     0.258   52  \n",
       "758                     0.197   26  \n",
       "759                     0.278   66  \n",
       "760                     0.766   22  \n",
       "761                     0.403   43  \n",
       "762                     0.142   33  \n",
       "763                     0.171   63  \n",
       "764                     0.340   27  \n",
       "765                     0.245   30  \n",
       "766                     0.349   47  \n",
       "767                     0.315   23  \n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOTANGER\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model with data\n",
    "logreg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Model Evaluation using Confusion Matrix </b> <br />\n",
    "A confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[119,  11],\n",
       "       [ 26,  36]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see the confusion matrix in the form of the array object. The dimension of this matrix is 2*2 because this model is binary classification. You have two classes 0 and 1. Diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions. In the output, 119 and 36 are actual predictions, and 26 and 11 are incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <b> Visualizing Confusion Matrix using Heatmap <b/> </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 257.44, 'Predicted label')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Confusion Matrix Evaluation Metrics </b> <br />\n",
    "Let's evaluate the model using model evaluation metrics such as accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8072916666666666\n",
      "Precision: 0.7659574468085106\n",
      "Recall: 0.5806451612903226\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, you got a classification rate of 80%, considered as good accuracy.\n",
    "\n",
    "Precision: Precision is about being precise, i.e., how accurate your model is. In other words, you can say, when a model makes a prediction, how often it is correct. In your prediction case, when your Logistic Regression model predicted patients are going to suffer from diabetes, that patients have 76% of the time.\n",
    "\n",
    "Recall: If there are patients who have diabetes in the test set and your Logistic Regression model can identify it 58% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> ROC Curve </b> <br />\n",
    "Receiver Operating Characteristic(ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 7 features per sample; expecting 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-7dd8da53b89a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0my_pred_proba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mauc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_proba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtpr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"data 1, auc=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                                 self.solver == 'liblinear')))\n\u001b[0;32m   1653\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0movr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1654\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1655\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m             \u001b[0mdecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \"\"\"\n\u001b[1;32m--> 303\u001b[1;33m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m         \u001b[0mexpit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 270\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 7 features per sample; expecting 8"
     ]
    }
   ],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC score for the case is 0.86. AUC score 1 represents perfect classifier, and 0.5 represents a worthless classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
